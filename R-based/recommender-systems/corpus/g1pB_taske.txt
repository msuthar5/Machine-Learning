In mathematics and computer science, dynamic programming is a method of sol
ving problems that exhibit the properties of overlapping sub problems and o
ptimal substructure. The term was originally used in the 1940s by Richard B
ellman to describe the process of solving problems where one needs to find 
the best decisions one after another. By 1953, he had refined this to the m
odern meaning. Bellman's contribution is remembered in the name of the Bell
man equation, a central result of dynamic programming which restates an opt
imization problem in recursive form. The word "programming" in "dynamic pro
gramming" has no particular connection to computer programming at all, and 
instead comes from the term "mathematical programming", a synonym for optim
ization. Thus, the "program" is the optimal plan for action that is produce
d. For instance, a finalized schedule of events at an exhibition is sometim
es called a program. Programming, in this sense, means finding an acceptabl
e plan of action, an algorithm.
Dynamic programming usually takes one of two approaches, the top-down appro
ach, the problem is broken into sub problems, and these sub problems are so
lved and the solutions remembered, in case they need to be solved again. Th
is is recursion and memorization combined together and the bottom-up approa
ch, all sub problems that might be needed are solved in advance and then us
ed to build up solutions to larger problems. This approach is slightly bett
er in stack space and number of function calls, but it is sometimes not int
uitive to figure out all the sub problems needed for solving the given prob
lem.
Some programming languages can automatically memorize the result of a funct
ion call with a particular set of arguments, in order to speed up call-by-n
ame. Some languages make it possible portably (e.g. Scheme, Common Lisp or 
Perl), some need special extensions.This is only possible for a referential
ly transparent function.

