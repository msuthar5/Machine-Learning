u_data <- read.csv("u.data", sep = ",", header = FALSE)
View(u_data)
u_data <- read.csv("u.data", sep = "", header = FALSE)
col_names(u_data) <- c("user id","item id" , "rating", "timestamp")
u_data_col_names <- c("user id","item id" , "rating", "timestamp")
col_names(u_data) <- u_data_col_names
colnames(u_data) <- u_data_col_names
u_items <- read.csv("u.item", sep = "", header = FALSE)
View(u_items)
u_items <- read.csv("u.item", sep = "|", header = FALSE)
u_items_col_names <- c("movie id", "movie title", "release date", "video release date", "IMDb URL", "unknown", "Action", "Adventure", "Animation" ,"Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", "Mystery", "Romance" ,"Sci-Fi", "Thriller", "War", "Western")
colnames(u_items)<- u_items_col_names
nrow(u_data)
for (i in range(0:10)){
print(i)
}
for (i in range(0,10)){
print(i)
}
i <- 0
for (i in range(0,10)){
print(i)
}
i <- 0
for (i in range(0,10)){
print(i)
i++
}
i <- 0
for (i in range(0,10)){
print(i)
i = i + 1
}
for (i in 1:nrow(u_data)) {
if (u_data[i,1] == 200) {
user200 = u_data[i,2]
}
}
user200
for (i in 1:nrow(u_data)) {
if (u_data[i,1] == 200) {
user200 = c(u_data[i,2])
}
}
user200
list200=c()
list200=c()
for(i in 1:100000)
{
if (u_data[i,1]==200)
{
list200=c(list200,u_data[i,2])
}
}
list200
length(list200)
i <- 0
for (i in 1:nrow(u_data)) {
if (u_data[i,1] == 200) {
user200 = c(u_data[i,2])
}
}
user200
for (i in 100000) {
if (u_data[i,1] == 200) {
#user200 = c(u_data[i,2])
append(user200, u_data[i,2])
}
}
user200
i <- 0
i <- 0
for (i in 100000) {
if (u_data[i,1] == 200) {
#user200 = c(u_data[i,2])
append(user200, u_data[i,2])
}
}
user200
i <- 0
for (i in 100000) {
if (u_data[i,1] == 200) {
#user200 = c(u_data[i,2])
#append(user200, u_data[i,2])
user200 = c(user200, u_data[i,2])
}
}
for (i in 1:100000) {
if (u_data[i,1] == 200) {
#user200 = c(u_data[i,2])
#append(user200, u_data[i,2])
user200 = c(user200, u_data[i,2])
}
}
length(user200)
user200 <- c()
for (i in 1:100000) {
if (u_data[i,1] == 200) {
#user200 = c(u_data[i,2])
#append(user200, u_data[i,2])
user200 = c(user200, u_data[i,2])
}
}
user200
length(user200)
movies200 <- c()
for (i in user200) {
movies200 <- (movies200, u_items[i,6:24])
}
movies200 <- c()
for (i in user200) {
movies200 <- c(movies200, u_items[i,6:24])
}
movie2000=c()
for (i in list200)
{
movie2000=c(movie2000, u_item[i,6:24])
}
movie2000=c()
for (i in list200)
{
movie2000=c(movie2000, u_items[i,6:24])
}
movies.matrix <- matrix(
nrow = length(user200),
ncol = 19,
byrow = TRUE
)
movies.matrix
movies.matrix
movie2000.matrix = matrix(
as.numeric(movie2000),
nrow=length(list200),
ncol=19,
byrow=T
)
movie2000.matrix
movies.matrix
movies.matrix <- matrix(
data = as.numeric(movie200),
nrow = length(user200),
ncol = 19,
byrow = TRUE
)
movies.matrix
movies.matrix <- matrix(
data = as.numeric(movies200),
nrow = length(user200),
ncol = 19,
byrow = TRUE
)
movies.matrix
genre200 <- apply(movies.matrix, 2, mean)
genre200
genre2_2000<-apply(movie2000.matrix,2,mean)
genre2_2000
user50 <- c()
for (i in 1:100000) {
if (u_data[i,1] == 50) {
user50 = c(user50, u_data[i,2])
}
}
movies50 <- c()
for (i in user50) {
movies50 <- c(movies50, u_items[i,6:24])
}
movies.matrix50 <- matrix(
data = as.numeric(movies50),
nrow = length(user50),
ncol = 19,
byrow = TRUE
)
movies.matrix50
genre50 <- apply(movies.matrix50, 2, mean)
genre50
movies.matrix50
movies50 <- c()
for (i in user50) {
movies50 <- c(movies50, u_items[i,6:24])
}
movies.matrix50 <- matrix(
data = as.numeric(movies50),
nrow = length(user50),
ncol = 19,
byrow = TRUE
)
movies.matrix50
genre200
genre50
cosine(1,1)
cosine.similarity(1,1)
install.packages("repOverlap")
library(repOverlap)
cosine(1,1)
cosine(genre_200,genre_50)
cosine(genre200,genre50)
cosine(genre2_200,genre2_50)
cosine(genre200,genre50)
movie127 <- as.numeric(u_items[127, 6:24 ])
cosine(movie127, genre200)
cosine(movie127, genre50)
utility_matrix <- matrix(
nrow = length(m_ids),
ncol = length(u_ids)
)
u_ids <- c(1,21,44,59,72,82,102,234,268,409,486)
m_ids <- c(1,2,3,4,5,6)
utility_matrix <- matrix(
nrow = length(m_ids),
ncol = length(u_ids)
)
utility_matrix
average_link <- hclust(dist(cleaned_df), method = "average")
most_pure = fviz_dend(average_link, show_labels = TRUE, main = "Average Linkage")
most_pure
mean_movie_1 <- sum(utility_matrix[1], ) / 10
mean_movie_1 <- sum(utility_matrix[1, ]) / 10
mean_movie_1 <- sum(utility_matrix[1, ]) / 10
mean_movie_2 <- sum(utility_matrix[2, ]) / 10
mean_movie_3 <- sum(utility_matrix[3, ]) / 10
mean_movie_4 <- sum(utility_matrix[4, ]) / 10
mean_movie_5 <- sum(utility_matrix[5, ]) / 10
mean_movie_6 <- sum(utility_matrix[6, ]) / 10
mean_centered_matrix <- matrix(
ncol = 11,
nrow = 6,
byrow = T
)
for (i in c(1:6))
{
mean_centered_matrix[i,1]<-cosine(utility_matrix[5,],utility_matrix[i,])
}
mean_centered_matrix
for (i in 1:6)
{
mean_centered_matrix[i,1]<-cosine(utility_matrix[5,],utility_matrix[i,])
}
mean_centered_matrix
mean_centered_matrix <- matrix(
ncol = 1,
nrow = 6,
byrow = T
)
for (i in 1:6)
{
mean_centered_matrix[i,1]<-cosine(utility_matrix[5,],utility_matrix[i,])
}
mean_centered_matrix
for (idx in 1:6)
{
mean_centered_matrix[idx,1]<-cosine(utility_matrix[5,],utility_matrix[idx,])
}
mean_centered_matrix
mean_centered_matrix <- matrix(
ncol = 1,
nrow = 6,
byrow = T
)
for (idx in 1:6)
{
mean_centered_matrix[idx,1]<-cosine(utility_matrix[5,],utility_matrix[idx,])
}
mean_centered_matrix
mean_centered_matrix <- matrix(
ncol = 1,
nrow = 6,
)
for (idx in 1:6)
{
mean_centered_matrix[idx,1] <- cosine(utility_matrix[5,],utility_matrix[idx,])
}
mean_centered_matrix
utility_matrix <- matrix(
c(5,5,4,2,4,4,3,3,3,0,4,3,0,0,0,3,0,2,2,2,0,0,4,0,0,4,0,2,0,0,1,0,2,3,0,0,4,0,0,2,4,4,0,0,3,2,4,0,4,0,3,3,0,0,0,5,0,0,0,0,0,0,0,0,4,4),
ncol = 11,
nrow = 6,
byrow = T
)
for (idx in 1:6)
{
mean_centered_matrix[idx,1] <- cosine(utility_matrix[5,],utility_matrix[idx,])
}
mean_centered_matrix
cosine(mean_centered_matrix[5, ], mean_centered_matrix[1, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[1, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[2, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[3, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[4, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[5, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[6, ])
mean_centered_matrix
mean_centered_matrix[1,]
mean_centered_matrix[2,]
mean_centered_matrix[3,]
mean_centered_matrix[5,]
cosine(mean_centered_matrix[5, ], mean_centered_matrix[3, ])
cosine(mean_centered_matrix[2, ], mean_centered_matrix[3, ])
cosine(mean_centered_matrix[2, ], mean_centered_matrix[3, ])
mean_centered_matrix[2,]
mean_centered_matrix[3,]
x <- mean_centered_matrix[2,]
y <- mean_centered_matrix[1,]
x
y
cosine(x,y)
cosine(0.5,0.44)
cosine(1dssd,1)
cosine(1444,1)
files <- list.files("Desktop/fall2017/422/labs/lab4/corpus/", full.names = TRUE)
minhash <- minhash_generator(n = 160,seed = 100)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
#tokens(corpus)
# This loop calculates the total number of shingles/tokens for all the documents
i <- 1
sum <- 0
while (i <= 100) {
sum <- sum + length(tokens(corpus[[i]]))
#print(tokens(corpus[[i]]))
i = i + 1
}
#Sum holds the total number of shingles at 5-shingle tokens
sum
orig_taske <- tokens(corpus[["orig_taske"]])
orig_taske[0:5]
per_red <-100 - (240 / sum * 100)
sprintf("The percent reduction is %f Precent" , (per_red))
# At 240 signatures (or hashes) we want a probability of 0.888 of getting a candidate pair in at least one band at
# a Jaccard similarity of 0.3 and above. How many bands will you need to get such a probability?
# Using the formula provided, we need 80 BANDS
# Using the number of bands you determined in (e), run LSH and find candidate pairs. How many candidate
# pairs do you get?
buckets <- lsh(corpus, b = 80)
lsh_probability(h = 240, b =  80, s = 0.3)
candidate_pairs <- lsh_candidates(buckets)
summary(candidate_pairs)
# here we compute the similarity scores for the candidate pairs with jaccard similarity
candidate_pairs <- lsh_compare(candidate_pairs, corpus, jaccard_similarity)
summary(candidate_pairs)
# Here we order the candidate pairs by decreasing similarity
# The below line of code orders
ordered_candidate_pairs_by_index <- order(candidate_pairs$score, decreasing = TRUE)
top_pairs <- candidate_pairs[ordered_candidate_pairs_by_index,]
top_pairs[1:5, ]
cosine(genre200,genre50)
movie127 <- as.numeric(u_items[127, 6:24 ])
cosine(movie127, genre200)
cosine(movie127, genre50)
user268rating <- (((cosine(mean_centered_matrix[5, ], mean_centered_matrix[2, ])*2)+(cosine(mean_centered_matrix[5, ], mean_centered_matrix[4, ])*4)+(cosine(mean_centered_matrix[5, ], mean_centered_matrix[3, ])*1))/(cosine(mean_centered_matrix[5, ], mean_centered_matrix[2, ])+cosine(mean_centered_matrix[5, ], mean_centered_matrix[4, ])+cosine(mean_centered_matrix[5, ], mean_centered_matrix[3, ])))
print(user268rating)
cosine(mean_centered_matrix[5, ], mean_centered_matrix[1, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[2, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[3, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[4, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[5, ])
cosine(mean_centered_matrix[5, ], mean_centered_matrix[6, ])
print(movieRating)
movieRating <- (
((cosine(mean_centered_matrix[5, ],
mean_centered_matrix[2, ])*2)+(cosine(mean_centered_matrix[5, ],
mean_centered_matrix[4, ])*4)+(cosine(mean_centered_matrix[5, ],
mean_centered_matrix[3, ])*1))/(cosine(mean_centered_matrix[5, ],
mean_centered_matrix[2, ])+cosine(mean_centered_matrix[5, ],
mean_centered_matrix[4, ])+cosine(mean_centered_matrix[5, ],
mean_centered_matrix[3, ])))
library(cluster)
library(factoextra)
library(textreuse)
library(fpc)
library(plyr)
library(lsa)
uncleaned_df <- read.csv("https://people.sc.fsu.edu/~jburkardt/datasets/hartigan/file46.txt", header = FALSE, sep = "", comment.char = "#")
# Here we clean the data frame to prepare to use it for analysis
uncleaned_df <- uncleaned_df[-c(1:4),]
row.names(uncleaned_df) <- uncleaned_df[,1]
cleaned_df <- uncleaned_df[,-1]
#cleaned_df <- scale(cleaned_df[,1:10])
col_names <- c('FI', 'SW', 'DA', 'NO', 'EN', 'GE', 'DU', 'FL', 'FR', 'IT', 'SP', 'PO')
colnames(cleaned_df) <- col_names
# Dendogram for Single Linkage
single_link <- hclust(dist(cleaned_df), method = "single")
fviz_dend(single_link, show_labels = TRUE, main = "Single Linkage")
"Singleton Clusters for Single-Linkage: (Great Britian, Ireland), (West Germany, Austria), (Luxemberg, Switzerland), (France, Belgium), (Denmark, Norway)"
# Dendogram for Complete Linkage
complete_link <- hclust(dist(cleaned_df), method = "complete")
fviz_dend(complete_link, show_labels = TRUE, main = "Complete Linkage")
"Singleton Clusters for Complete-Linkage: (Great Britian, Ireland), (West Germany, Austria), (Luxemberg, Switzerland), (France, Belgium), (Denmark, Norway)"
# Dendogram for Average Linkage
average_link <- hclust(dist(cleaned_df), method = "average")
most_pure = fviz_dend(average_link, show_labels = TRUE, main = "Average Linkage")
most_pure
"Singleton Clusters for Average-Linkage: (Great Britian, Ireland), (West Germany, Austria), (Luxemberg, Switzerland), (France, Belgium), (Denmark, Norway), (Portugal, Spain)"
"Italy should be clustered with the Complete Linkage strategy. In complete linkage,
Italy is clustered with France and Belgium. All 3 of these countries tend to speak
roughly 3 languages (where a vast majority speaking a single language) as opposed of a country like Denmark, where
a multitude of languages are spoken by the population. Because the data set shows that Italians speak Italian with a high majority
and only have a sparse amount of other languages spoken, it makes sense that Italy
should be clustered with other similiar countries such as  Belgium and France"
"Most pure cluster is the Average Link Cluster as it has 6 2-singleton clusters"
fviz_dend(average_link, show_labels = TRUE, main = "Average Linkage", h=125)
"At a height of 125, we see 8 clusters"
# We will now re-cluster with all linkage modes with k = 8
# Dendogram for Single Linkage
single_link_k8 <- eclust(cleaned_df, "hclust", k = 8, hc_method="single")
fviz_dend(single_link_k8, show_labels = TRUE, main = "Single Linkage")
#stats_singlek8 <- cluster.stats(dist(cleaned_df), single_link_k8$cluster, silhouette = TRUE)
# Dendogram for Complete Linkage
complete_link_k8 <- eclust(cleaned_df, "hclust", k = 8, hc_method="complete")
fviz_dend(complete_link_k8, show_labels = TRUE, main = "Complete Linkage")
# Dendogram for Average Linkage
average_link_k8 <- eclust(cleaned_df, "hclust", k = 8, hc_method="average")
fviz_dend(average_link_k8, show_labels = TRUE, main = "Average Linkage")
# Here We will compute the Dunn and Silhouette for each linkage method
# Single Linkage
stats_singlek8 <- cluster.stats(dist(cleaned_df), single_link_k8$cluster, silhouette = TRUE)
stats_singlek8$avg.silwidth
stats_singlek8$dunn
# Complete Linkage
stats_completek8 <- cluster.stats(dist(cleaned_df), complete_link_k8$cluster, silhouette = TRUE)
stats_completek8$avg.silwidth
stats_completek8$dunn
# Average Linkage
stats_averagek8 <- cluster.stats(dist(cleaned_df), average_link_k8$cluster, silhouette = TRUE)
stats_averagek8$avg.silwidth
stats_averagek8$dunn
# Print the average silhouette width
stats_singlek8$avg.silwidth
stats_completek8$avg.silwidth
stats_averagek8$avg.silwidth
# Print the Dunn index
stats_singlek8$dunn
stats_completek8$dunn
stats_averagek8$dunn
"According to the Dunn index, the best cluster is the one produced by
average linkage as it has the highest dunn index value"
"According to the average silhouette metric, the best cluster is the one produced
by complete linkage as it has the highest avg silhouette value"
files <- list.files("Desktop/fall2017/422/labs/lab4/corpus/", full.names = TRUE)
minhash <- minhash_generator(n = 160,seed = 100)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
#tokens(corpus)
# This loop calculates the total number of shingles/tokens for all the documents
i <- 1
sum <- 0
while (i <= 100) {
sum <- sum + length(tokens(corpus[[i]]))
#print(tokens(corpus[[i]]))
i = i + 1
}
#Sum holds the total number of shingles at 5-shingle tokens
sum
# Dimensions of matrix --> 22033 x 100
# Print the first 5 shingles (or tokens) of the file orig_taske.txt.
orig_taske <- tokens(corpus[["orig_taske"]])
orig_taske[0:5]
# We will fix our signatures (or hashes, or the rows in the signature matrix) at 240. This represents what
# percentage reduction in the size of the problem?
per_red <-100 - (240 / sum * 100)
sprintf("The percent reduction is %f Precent" , (per_red))
# At 240 signatures (or hashes) we want a probability of 0.888 of getting a candidate pair in at least one band at
# a Jaccard similarity of 0.3 and above. How many bands will you need to get such a probability?
# Using the formula provided, we need 80 BANDS
# Using the number of bands you determined in (e), run LSH and find candidate pairs. How many candidate
# pairs do you get?
buckets <- lsh(corpus, b = 80)
lsh_probability(h = 240, b =  80, s = 0.3)
candidate_pairs <- lsh_candidates(buckets)
summary(candidate_pairs)
# here we compute the similarity scores for the candidate pairs with jaccard similarity
candidate_pairs <- lsh_compare(candidate_pairs, corpus, jaccard_similarity)
summary(candidate_pairs)
# Here we order the candidate pairs by decreasing similarity
# The below line of code orders
ordered_candidate_pairs_by_index <- order(candidate_pairs$score, decreasing = TRUE)
top_pairs <- candidate_pairs[ordered_candidate_pairs_by_index,]
# (g) Sort the candidate pairs according to their score field, in descending order
#(i.e., from highest score to lowest score). List the top 5 candidates that are similar
top_pairs[1:5, ]
top_pairs <- candidate_pairs[ordered_candidate_pairs_by_index,]
buckets <- lsh(corpus, b = 80)
lsh_probability(h = 240, b =  80, s = 0.3)
candidate_pairs <- lsh_candidates(buckets)
summary(candidate_pairs)
candidate_pairs <- lsh_candidates(buckets)
per_red <-100 - (240 / sum * 100)
sprintf("The percent reduction is %f Precent" , (per_red))
buckets <- lsh(corpus, b = 80)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
files <- list.files("Desktop/fall2017/422/labs/lab4/corpus/", full.names = TRUE)
minhash <- minhash_generator(n = 160,seed = 100)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
files <- list.files("Desktop/fall2017/422/labs/lab4/corpus/", full.names = TRUE)
minhash <- minhash_generator(n = 160,seed = 100)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
#tokens(corpus)
# This loop calculates the total number of shingles/tokens for all the documents
i <- 1
sum <- 0
while (i <= 100) {
sum <- sum + length(tokens(corpus[[i]]))
#print(tokens(corpus[[i]]))
i = i + 1
}
files <- list.files("Desktop/fall2017/422/labs/lab4/corpus/", full.names = TRUE)
minhash <- minhash_generator(n = 160,seed = 100)
corpus <- TextReuseCorpus(files, tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash, keep_tokens = TRUE)
clear(setwd())
